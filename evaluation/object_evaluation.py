"""
Command: CUDA_VISIBLE_DEVICES=1, python -m evaluation.object_evaluation --settings_file config/settings.yaml
             --checkpoint_file <path>
             --gt_dir <path>

"""
import os
import tqdm
import torch
import argparse
import numpy as np
import matplotlib

import models.yolov3_modules as yolov3_modules
from utils import utils, viz_utils
from config.settings import Settings
from utils.saver import CheckpointSaver
from utils import object_detection_eval
from training.object_det_trainer import ObjectDetModel
from evaluation.mvsec_evaluation import MVSECEvaluation


class ObjectEvaluation(ObjectDetModel):
    """Evalutes the output txt file generated by the code provided with the Network Grafting approach"""
    def __init__(self, settings, checkpoint_file, visualize, visualize_path, ground_truth_path=None,
                 use_settings_a=False):
        self.settings = settings
        self.visualize = visualize
        self.use_settings_a = use_settings_a
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        self.buildModels()

        if self.use_settings_a:
            self.dataset_name = self.settings.dataset_name_a
            self.test_loader_sensor = self.createDataset(self.settings.dataset_name_a,
                                                         self.settings.dataset_path_a,
                                                         self.settings.img_size_a,
                                                         1,
                                                         self.settings.nr_events_window_a,
                                                         self.settings.event_representation_a,
                                                         self.settings.input_channels_a // 2)
            weight_list = self.getModelList('sensor_a')
            print('For the test evaluation, the parameters for sensor a are taken from the provided yaml file')
        else:
            self.dataset_name = self.settings.dataset_name_b
            self.test_loader_sensor = self.createDataset(self.settings.dataset_name_b,
                                                         self.settings.dataset_path_b,
                                                         self.settings.img_size_b,
                                                         20,
                                                         self.settings.nr_events_window_b,
                                                         self.settings.event_representation_b,
                                                         self.settings.input_channels_b // 2)
            weight_list = self.getModelList('sensor_b')
            print('For the test evaluation, the parameters for sensor b are taken from the provided yaml file')

        self.models_dict = {k: v.to(self.device) for k, v in self.models_dict.items()}
        self.saver = CheckpointSaver(save_dir=None)

        self.saver.load_pretrained_weights(self.models_dict, weight_list, checkpoint_file=checkpoint_file)
        self.gt_bboxes = []
        self.det_bboxes = []
        self.epoch_count = 0
        self.image_count = 0

        if ground_truth_path is not None:
            self.mvsec_evaluator = MVSECEvaluation(ground_truth_path)

        if self.visualize:
            self.visualize_path = visualize_path

    def getModelList(self, sensor_name):
        self.sensor = sensor_name
        return ["front_" + sensor_name, "front_shared", 'back_end']

    def evaluate(self):
        with torch.no_grad():
            for model in self.models_dict:
                self.models_dict[model].eval()

            val_dataset_length = self.test_loader_sensor.__len__()
            self.pbar = tqdm.tqdm(total=val_dataset_length, unit='Batch', unit_scale=True)

            for i_batch, sample_batched in enumerate(self.test_loader_sensor):
                self.pbar.update(1)
                self.epoch_count += 1
                self.testBatchStep(sample_batched)

        self.computeMeanAveragePrecision()
        if (self.settings.dataset_name_b == 'MVSEC_events' and not self.use_settings_a) or  \
                (self.settings.dataset_name_a == 'MVSEC_events' and self.use_settings_a):
            self.computeMVSECDontCaremAP()

    def testBatchStep(self, sample_batched):
        sample_batched = [tensor.to(self.device) for tensor in sample_batched]
        data = sample_batched[0]
        labels = sample_batched[1]

        gen_model = self.models_dict['front_' + self.sensor]
        task_backend = self.models_dict["back_end"]

        content_features, _, _, _ = gen_model(data)

        if labels.shape[0] == 0:
            loss_pred, pred, step_metric = task_backend(content_features, None)
        else:
            loss_pred, pred, step_metric = task_backend(content_features, labels)

        detected_bboxes = self.extractPredictedBBoxNMS(pred, self.settings.img_size_b,
                                                       iou_threshold=0.2)

        self.addBBoxmAP(detected_bboxes.cpu().numpy(), labels.cpu().numpy(), nr_imgs=data.shape[0])

        if self.visualize:
            self.visualizeGtPredictedBbox(detected_bboxes, labels, data, step=self.epoch_count, tag='test',
                                          nms_extracted_bbox=True)

    def extractPredictedBBoxNMS(self, pred, img_shape, iou_threshold=0.5, pred_confidence=0.5):
        """
        Extracts the bounding boxes with an objectness score above 0.5 and applies non maximum suppression

        :return filtered_boxes: [img_id, x_min, y_min, x_max. y_max, object_score, pred_class_id, class_score]
        """
        nr_img, nr_bbox = pred.shape[:2]
        detected_bboxes_bool = pred[:, :, 4] > pred_confidence  # 0.5  # Object score above 0.5

        if detected_bboxes_bool.sum() == 0:
            return torch.zeros([0, 8])

        img_id = torch.arange(end=nr_img, device=pred.device)[:, None].expand([-1, nr_bbox])
        detected_bboxes = pred[detected_bboxes_bool]
        detected_bboxes = utils.cropBboxToFrame(detected_bboxes, image_shape=img_shape)

        detected_bboxes_img_ids = img_id[detected_bboxes_bool]
        pred_class_score, pred_class = torch.max(detected_bboxes[:, 5:], dim=1)

        detected_bboxes = torch.cat([detected_bboxes_img_ids[:, None],
                                     detected_bboxes[:, :5],
                                     pred_class[:, None],
                                     pred_class_score[:, None]], dim=-1)

        filtered_bboxes = yolov3_modules.nonMaxSuppression(detected_bboxes, iou=iou_threshold)

        return filtered_bboxes

    def addBBoxmAP(self, detected_bboxes, labels, nr_imgs):
        """Adds the detected bboxes and labels to the corresponding lists"""
        det_bboxes = [[] for _ in range(nr_imgs)]
        gt_bboxes = [[] for _ in range(nr_imgs)]

        eval_bbox = detected_bboxes[:, [0, 1, 2, 3, 4, 6, 7]]
        eval_bbox[:, 3:5] = eval_bbox[:, 1:3] + eval_bbox[:, 3:5]

        for i_bbox, det_bbox in enumerate(eval_bbox):
            # det_bbox: [img_id, x_min, y_min, x_max, y_max, class_id, class_score]
            det_bboxes[int(det_bbox[0])].append(eval_bbox[i_bbox, 1:])

        labels[:, 3:5] = labels[:, 1:3] + labels[:, 3:5]
        # print(labels)
        for i_bbox, gt_bbox in enumerate(labels):
            # # Gt Bboxes: [x_min, y_min, x_max, y_max, class_id]
            gt_bboxes[int(gt_bbox[0])].append(gt_bbox[1:])

        self.gt_bboxes = self.gt_bboxes + gt_bboxes
        self.det_bboxes = self.det_bboxes + det_bboxes

    def computeMeanAveragePrecision(self):
        """Computes the average precision for each class as well as the mean average precision"""
        out = object_detection_eval.evaluatePascalVOCMetrics(self.gt_bboxes, self.det_bboxes,
                                                             nr_classes=len(self.object_classes))
        class_AP = np.zeros(len(self.object_classes), dtype=np.float)

        for i_class, class_name in enumerate(self.object_classes):
            class_AP[i_class] = out[i_class]['AP']
            print('------ %s AP ------' % class_name)
            print(out[i_class]['AP'])
            print('Total Number of Positives       %s' % out[i_class]['total positives'])
            print('Total Number of True Positives  %s' % out[i_class]['total TP'])
            print('Total Number of False Positives %s' % out[i_class]['total FP'])

        print('----- mAP ------')
        print(np.mean(class_AP))
        print('----- - ------')

    def computeMVSECDontCaremAP(self):
        ground_truth_bbox, dont_care_bbox = self.mvsec_evaluator.extractGroundTruth()

        # Scale Bounding Boxes
        new_gt_ground_truth_bboxes = self.scaleBoundingBoxes(ground_truth_bbox)
        new_dont_care_bbox = self.scaleBoundingBoxes(dont_care_bbox)

        print(' ---- Don`t Care ----')
        self.mvsec_evaluator.evaluate(predicted_bbox=self.det_bboxes,
                                      ground_truth_bbox=new_gt_ground_truth_bboxes,
                                      dont_care_bbox=new_dont_care_bbox)

    def scaleBoundingBoxes(self, ground_truth_bbox):
        original_shape_hw = (198, self.test_dataset.original_width)
        target_ratio = float(self.test_dataset.height) / float(self.test_dataset.width)
        unscaled_target_height = int(original_shape_hw[1] * target_ratio)
        cropped_height = int(original_shape_hw[0] - unscaled_target_height)

        new_gt_ground_truth_bboxes = []
        for i, gt_bbox in enumerate(ground_truth_bbox):
            if len(gt_bbox) == 0:
                new_gt_ground_truth_bboxes.append(gt_bbox)
                continue
            bbox_array = self.test_dataset.scale_bounding_boxes(np.array(gt_bbox),
                                                                (original_shape_hw[1], unscaled_target_height),
                                                                cropped_height)
            bbox_array[:, [2, 3]] = bbox_array[:, [2, 3]] + bbox_array[:, :2]

            new_gt_ground_truth_bboxes.append(list(bbox_array))

        return new_gt_ground_truth_bboxes

    def visualizeGtPredictedBbox(self, detected_bbox, labels, input_data, step, tag='train', nms_extracted_bbox=False):
        """Visualizes the detected bounding box as well as the ground truth bounding boxes for the first image"""
        rgb_img = viz_utils.visualizeTensors(input_data[0, None, ...]).squeeze(0)
        viz_img = rgb_img.cpu().numpy().transpose([1, 2, 0])

        gt_bbox = labels[labels[:, 0] == 0]
        viz_img = viz_utils.drawBoundingBoxes(viz_img,
                                              bounding_boxes=gt_bbox[:, 1:5].int().cpu().numpy(),
                                              class_name=[self.object_classes[label] for label in gt_bbox[:, -1]],
                                              ground_truth=True)

        if viz_img.sum() < 10:
            return
        if detected_bbox.sum() == 0:
            rgb_grid = torch.from_numpy(viz_img.transpose([2, 0, 1]).astype(float))
            matplotlib.image.imsave(os.path.join(self.visualize_path,
                                                 'image_' + str(self.image_count) + '.png'),
                                    rgb_grid.cpu().numpy().transpose([1, 2, 0]))
            self.image_count += 1

            print('Empty Image saved')
            return

        if not nms_extracted_bbox:
            detected_bboxes_bool = detected_bbox[0, :, 4] > 0.5
            if detected_bboxes_bool.sum() == 0:
                # self.img_summaries(tag, viz_img.transpose([2, 0, 1]).astype(float), step)
                return

            bbox_to_viz = detected_bbox[0, detected_bboxes_bool, :4].int()
            bbox_to_viz = utils.cropBboxToFrame(bbox_to_viz, image_shape=self.settings.img_size_b).int().cpu().numpy()
            class_pred = torch.argmax(detected_bbox[0, detected_bboxes_bool, 5:], dim=-1).int().cpu().numpy()
        else:
            detected_bbox = detected_bbox.cpu().numpy()
            detected_bbox = detected_bbox[np.equal(detected_bbox[:, 0], 0), 1:]
            if detected_bbox.shape[0] == 0:
                # self.img_summaries(tag, viz_img.transpose([2, 0, 1]).astype(float), step)

                return

            bbox_to_viz = detected_bbox[:, :4].astype(np.int)
            class_pred = detected_bbox[:, 5].astype(np.int)

        viz_img = viz_utils.drawBoundingBoxes(viz_img,
                                              bounding_boxes=bbox_to_viz,
                                              class_name=[self.object_classes[pred] for pred in class_pred],
                                              ground_truth=False)

        rgb_grid = torch.from_numpy(viz_img.transpose([2, 0, 1]).astype(float))
        matplotlib.image.imsave(os.path.join(self.visualize_path,
                                             'image_' + str(self.image_count) + '.png'), rgb_grid.cpu().numpy().transpose([1, 2, 0]))
        self.image_count += 1


    def getDataloader(self, dataset_name):
        """Returns the dataset loader specified in the settings file"""
        if dataset_name == 'MVSEC_events':
            from datasets.mvsec_loader import MVSEC_Events
            return MVSEC_Events
        elif dataset_name == 'OneMpProphesee_events':
            from datasets.oneMP_prophesee_loader import OneMPProphesee
            return OneMPProphesee
        elif dataset_name == 'Waymo_gray':
            from datasets.waymo_loader import WaymoGray
            return WaymoGray

    def createDataset(self, dataset_name, dataset_path, img_size, batch_size, nr_events_window, event_representation,
                      nr_temporal_bins):
        """
        Creates the validation data based on the provided paths and parameters.
        """
        dataset_builder = self.getDataloader(dataset_name)

        mode = 'test'
        if dataset_name in ['Waymo_gray']:
            mode = 'val'
            print('Validation dataset is taken for %s' % dataset_name)

        self.test_dataset = dataset_builder(dataset_path,
                                            height=img_size[0],
                                            width=img_size[1],
                                            nr_events_window=nr_events_window,
                                            augmentation=False,
                                            mode=mode,
                                            event_representation=event_representation,
                                            nr_temporal_bins=nr_temporal_bins,
                                            extension_dataset_path=None)

        self.object_classes = self.test_dataset.class_list

        from datasets.object_det_loader import ObjectDetLoader
        dataset_loader = ObjectDetLoader
        test_loader_sensor = dataset_loader(self.test_dataset, batch_size=batch_size,
                                            num_workers=self.settings.num_cpu_workers,
                                            pin_memory=False, shuffle=False, drop_last=False)

        return test_loader_sensor


def main():
    parser = argparse.ArgumentParser(description='Test network.')
    parser.add_argument('--settings_file', help='Path to settings yaml', required=True)
    parser.add_argument('--checkpoint_file', help='Path to checkpoint file to test', required=True)
    parser.add_argument('--gt_dir', help='Path to ground truth bounding boxes', required=False, default=None)
    parser.add_argument('--viz_dir', help='Path to directory, where the bounding box images are stored', required=False,
                        default='log/object_detection')

    parser.add_argument('--use_settings_a', help='Whether to take the settings for sensor a', action='store_true',
                        default=False)
    parser.add_argument('--visualize', help='If detection images should be stored', action='store_true',
                        default=False)
    args = parser.parse_args()
    settings_filepath = args.settings_file
    settings = Settings(settings_filepath, generate_log=False)

    if args.mvsec_reconst_path is not None:
        if args.use_settings_a:
            settings.dataset_name_a = 'Reconst_MVSEC_gray'
            settings.dataset_path_a = args.mvsec_reconst_path
        else:
            settings.dataset_name_b = 'Reconst_MVSEC_gray'
            settings.dataset_path_b = args.mvsec_reconst_path

    evaluator = ObjectEvaluation(settings, args.checkpoint_file, args.visualize, args.viz_dir, args.gt_dir,
                                 args.use_settings_a)
    evaluator.evaluate()


if __name__ == "__main__":
    main()
